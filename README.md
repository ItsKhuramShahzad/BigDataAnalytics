# BigDataAnalytics(Hadoop)
## 0	Task -0: Hadoop step Up and Installation in Google Cloud Engine:
   ### 0.1	ssh key generating.
   ### 0.2	Login with Instance
   ### 0.3	Update	<br />
   ### 0.4	Install open jdk	<br />
   ### 0.5	Creating new user for Hadoop	<br />
   #### 0.6	Download Hadoop	<br />
   ### 0.7	setting configuration	<br />
   ### 0.8	setting hdfs configuration	<br />
   ### 0.9	starting services	<br />
   ### 0.9	starting services	<br />

## 1	Task -1: Creating Your Directory Space:

## 2	Task -02:  Understanding the System: <br />
   ### 2.1	How many data nodes are part of the Hadoop topology?
   ### 2.2	What are the IP addresses of these datanodes?
   ### 2.3	What is the configured and present capacity of the HDFS?
   ### 2.4	What is the default file replication count?	
   ### 2.5	Task 02 steps:
   #### 2.5.1	Step a
   #### 2.5.1	Step b
   #### 2.5.1	Step c
   
## 3	Task -03:  Getting sample data: <br />
   ### 3.1	How to Upload file in Google Instances
   #### 3.1.1	Step1: In instances Click on SSH	
   #### 3.1.2	This screen will be shown
   #### 3.1.3	Click on upload icon
   #### 3.1.4	We can see that file uploaded successfully
   #### 3.1.5	Upload file from Instances to Hadoop user directory.
   #### 3.1.6	Now run hdfs fsck command to see detail about file
   #### 3.1.7	What is the default block size (in Mb) of the airline_data.csv file?
   #### 3.1.8	Is there any missing replicas for the file airline_data.csv?
   #### 3.1.9	What command will you use to change this block size to 6 Mb (remember to convert into bytes)
   #### 3.1.10	How many blocks are used by airline_data.csv after changing block size in Question 2?
   #### 3.1.11	How many missing replicas are there for file airline_data.csv after block change?
   #### 3.1.11	Why are there missing replicas?
   
