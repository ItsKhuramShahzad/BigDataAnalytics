# Big Data Analytics (Hadoop)
## 0	Task -0: Hadoop step Up and Installation in Google Cloud Engine:
   ### 0.1	ssh key generating.
   ### 0.2	Login with Instance
   ### 0.3	Update	<br />
   ### 0.4	Install open jdk	<br />
   ### 0.5	Creating new user for Hadoop	<br />
   ### 0.6	Download Hadoop	<br />
   ### 0.7	setting configuration	<br />
   ### 0.8	setting hdfs configuration	<br />
   ### 0.9	starting services	<br />
   ### 0.9	starting services	<br />

## 1	Task -1: Creating Your Directory Space:

## 2	Task -02:  Understanding the System: <br />
   ### 2.1	How many data nodes are part of the Hadoop topology?
   ### 2.2	What are the IP addresses of these datanodes?
   ### 2.3	What is the configured and present capacity of the HDFS?
   ### 2.4	What is the default file replication count?	
   ### 2.5	Task 02 steps:
   #### 2.5.1	Step a
   #### 2.5.1	Step b
   #### 2.5.1	Step c
   
## 3	Task -03:  Getting sample data: <br />
   ### 3.1	How to Upload file in Google Instances
   #### 3.1.1	Step1: In instances Click on SSH	
   #### 3.1.2	This screen will be shown
   #### 3.1.3	Click on upload icon
   #### 3.1.4	We can see that file uploaded successfully
   #### 3.1.5	Upload file from Instances to Hadoop user directory.
   #### 3.1.6	Now run hdfs fsck command to see detail about file
   #### 3.1.7	What is the default block size (in Mb) of the airline_data.csv file?
   #### 3.1.8	Is there any missing replicas for the file airline_data.csv?
   #### 3.1.9	What command will you use to change this block size to 6 Mb (remember to convert into bytes)
   #### 3.1.10	How many blocks are used by airline_data.csv after changing block size in Question 2?
   #### 3.1.11	How many missing replicas are there for file airline_data.csv after block change?
   #### 3.1.11	Why are there missing replicas?
   
## 4	Task -04:  Setting up First Map Reduce
   ### 3.1	How to Upload file in Google Instances
   ### 4.1	Mapper Code:
   ### 4.2	Reducer Code
   ### 4.3	Write and run locall
   ### 4.4	Uploaded files VM instances and its user.
   ### 4.5	To run Job in Hadoop:
   ### 4.6	What was the <key,value> pair used in this query?
   ### 4.7	How many mapper threads were used?
   ### 4.8	How many mapper threads were used?
   ### 4.9	What was the time spent by all mapper threads?
   ### 4.10	What was the time spent by all reducer threads?
   ### 4.11	What is the file name in which your output is located?
   ### 4.12	Variation 1:
   ### 4.13	Variation 2:
   ### 4.14	Variation 3:
   
 ## 5	Task -05:  Designing Additional Queries
   ### Task 5a: Present the Total Flights per Year as a percentage
   ### Task 5b: Which is the busiest month of airline traffic of all years?
   ### Task 5c: Which Airline Carrier has flown the most flights over the 10 year period?
   ### Task 5d: Which Airport has been the most busiest over the 10 year period?
   ### Task 5e: Which Airport has the Largest Flights to Cancellation Ratio?
   ### Task 5f: Find the Total Amount of Delay Minutes Grouped by Airline
   ### Task 5g: Find the Airport with most Cancelled Flights in 2016.
   ### Task 5h: Find the average delay time for an airport that is the most busiest of all other airports
   ### Task 5j: What is the Probability that a Flight will be Cancelled due to Bad Weather at the Most Busiest Airport of all other Airports?
   ### Task 5k: Design Any Query of your Choice (Output must be limited)
   
   
